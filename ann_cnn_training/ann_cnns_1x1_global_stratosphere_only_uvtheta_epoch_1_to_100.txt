NGPUS = 1
Training the 1x1 ANN-CNNs, global horizontal and stratosphere_only vertical model with features uvtheta with min-max learning rates 0.0001 to 0.0009 for a CyclicLR, and dropout=0.1.

Training the global horizontal and stratosphere_only vertical model, with features uvtheta with min-max learning rates 0.0001 to 0.0009, and dropout=0.1. Starting from epoch 1. Training on years [2010 2012 2014] and testing on years [2015].

Defined input files
train batch size = 20
validation batch size = 20
Input dim: 183, hidden dim: 732, output dim: 120
Restarting - model created. 
 --- model size: 9.55 MBs,
 --- Num params: 2.495 mil. 
Epoch 1, 1/100, training mseloss: 0.245479, testing mseloss: 0.247675
Epoch 2, 2/100, training mseloss: 0.239417, testing mseloss: 0.243364
Epoch 3, 3/100, training mseloss: 0.237617, testing mseloss: 0.242973
Epoch 4, 4/100, training mseloss: 0.236488, testing mseloss: 0.242486
Epoch 5, 5/100, training mseloss: 0.235657, testing mseloss: 0.239980
Epoch 6, 6/100, training mseloss: 0.234914, testing mseloss: 0.238075
Epoch 7, 7/100, training mseloss: 0.234268, testing mseloss: 0.236890
Epoch 8, 8/100, training mseloss: 0.233678, testing mseloss: 0.236854
Epoch 9, 9/100, training mseloss: 0.233246, testing mseloss: 0.237514
Epoch 10, 10/100, training mseloss: 0.232810, testing mseloss: 0.237696
Epoch 11, 11/100, training mseloss: 0.232389, testing mseloss: 0.236235
Epoch 12, 12/100, training mseloss: 0.231867, testing mseloss: 0.235038
Epoch 13, 13/100, training mseloss: 0.231450, testing mseloss: 0.234695
Epoch 14, 14/100, training mseloss: 0.231032, testing mseloss: 0.234826
Epoch 15, 15/100, training mseloss: 0.230675, testing mseloss: 0.235942
Epoch 16, 16/100, training mseloss: 0.230415, testing mseloss: 0.236096
Epoch 17, 17/100, training mseloss: 0.230045, testing mseloss: 0.235459
Epoch 18, 18/100, training mseloss: 0.229643, testing mseloss: 0.234366
Epoch 19, 19/100, training mseloss: 0.229314, testing mseloss: 0.234314
Epoch 20, 20/100, training mseloss: 0.228976, testing mseloss: 0.233957
Epoch 21, 21/100, training mseloss: 0.228750, testing mseloss: 0.234635
Epoch 22, 22/100, training mseloss: 0.228577, testing mseloss: 0.235195
Epoch 23, 23/100, training mseloss: 0.228344, testing mseloss: 0.234637
Epoch 24, 24/100, training mseloss: 0.228028, testing mseloss: 0.233630
Epoch 25, 25/100, training mseloss: 0.227808, testing mseloss: 0.233736
Epoch 26, 26/100, training mseloss: 0.227538, testing mseloss: 0.233419
Epoch 27, 27/100, training mseloss: 0.227337, testing mseloss: 0.233810
Epoch 28, 28/100, training mseloss: 0.227216, testing mseloss: 0.234219
Epoch 29, 29/100, training mseloss: 0.227056, testing mseloss: 0.234357
Epoch 30, 30/100, training mseloss: 0.226804, testing mseloss: 0.233742
Epoch 31, 31/100, training mseloss: 0.226627, testing mseloss: 0.233750
Epoch 32, 32/100, training mseloss: 0.226438, testing mseloss: 0.233247
Epoch 33, 33/100, training mseloss: 0.226286, testing mseloss: 0.233625
Epoch 34, 34/100, training mseloss: 0.226184, testing mseloss: 0.234084
Epoch 35, 35/100, training mseloss: 0.226051, testing mseloss: 0.234574
Epoch 36, 36/100, training mseloss: 0.225888, testing mseloss: 0.234158
Epoch 37, 37/100, training mseloss: 0.225732, testing mseloss: 0.233839
Epoch 38, 38/100, training mseloss: 0.225607, testing mseloss: 0.232865
Epoch 39, 39/100, training mseloss: 0.225463, testing mseloss: 0.233651
Epoch 40, 40/100, training mseloss: 0.225378, testing mseloss: 0.233541
Epoch 41, 41/100, training mseloss: 0.225296, testing mseloss: 0.233956
Epoch 42, 42/100, training mseloss: 0.225157, testing mseloss: 0.233581
Epoch 43, 43/100, training mseloss: 0.225004, testing mseloss: 0.233345
Epoch 44, 44/100, training mseloss: 0.224920, testing mseloss: 0.233004
Epoch 45, 45/100, training mseloss: 0.224806, testing mseloss: 0.233402
Epoch 46, 46/100, training mseloss: 0.224700, testing mseloss: 0.233549
Epoch 47, 47/100, training mseloss: 0.224681, testing mseloss: 0.233871
Epoch 48, 48/100, training mseloss: 0.224568, testing mseloss: 0.233531
Epoch 49, 49/100, training mseloss: 0.224404, testing mseloss: 0.233142
Epoch 50, 50/100, training mseloss: 0.224342, testing mseloss: 0.233111
Epoch 51, 51/100, training mseloss: 0.224258, testing mseloss: 0.233092
Epoch 52, 52/100, training mseloss: 0.224152, testing mseloss: 0.233389
Epoch 53, 53/100, training mseloss: 0.224138, testing mseloss: 0.233318
Epoch 54, 54/100, training mseloss: 0.224065, testing mseloss: 0.233208
Epoch 55, 55/100, training mseloss: 0.223932, testing mseloss: 0.232919
Epoch 56, 56/100, training mseloss: 0.223857, testing mseloss: 0.233060
Epoch 57, 57/100, training mseloss: 0.223785, testing mseloss: 0.232923
Epoch 58, 58/100, training mseloss: 0.223697, testing mseloss: 0.233449
Epoch 59, 59/100, training mseloss: 0.223685, testing mseloss: 0.233465
Epoch 60, 60/100, training mseloss: 0.223646, testing mseloss: 0.233562
Epoch 61, 61/100, training mseloss: 0.223549, testing mseloss: 0.233161
Epoch 62, 62/100, training mseloss: 0.223447, testing mseloss: 0.233350
Epoch 63, 63/100, training mseloss: 0.223399, testing mseloss: 0.232810
Epoch 64, 64/100, training mseloss: 0.223323, testing mseloss: 0.233434
Epoch 65, 65/100, training mseloss: 0.223285, testing mseloss: 0.233388
Epoch 66, 66/100, training mseloss: 0.223245, testing mseloss: 0.233406
Epoch 67, 67/100, training mseloss: 0.223172, testing mseloss: 0.232993
Epoch 68, 68/100, training mseloss: 0.223060, testing mseloss: 0.232879
Epoch 69, 69/100, training mseloss: 0.223043, testing mseloss: 0.232842
Epoch 70, 70/100, training mseloss: 0.222977, testing mseloss: 0.233089
Epoch 71, 71/100, training mseloss: 0.222912, testing mseloss: 0.233113
Epoch 72, 72/100, training mseloss: 0.222898, testing mseloss: 0.233169
Epoch 73, 73/100, training mseloss: 0.222826, testing mseloss: 0.233012
Epoch 74, 74/100, training mseloss: 0.222725, testing mseloss: 0.232850
Epoch 75, 75/100, training mseloss: 0.222703, testing mseloss: 0.232703
Epoch 76, 76/100, training mseloss: 0.222672, testing mseloss: 0.232972
Epoch 77, 77/100, training mseloss: 0.222591, testing mseloss: 0.232927
Epoch 78, 78/100, training mseloss: 0.222584, testing mseloss: 0.232967
Epoch 79, 79/100, training mseloss: 0.222534, testing mseloss: 0.233193
Epoch 80, 80/100, training mseloss: 0.222443, testing mseloss: 0.232596
Epoch 81, 81/100, training mseloss: 0.222402, testing mseloss: 0.232712
Epoch 82, 82/100, training mseloss: 0.222366, testing mseloss: 0.233098
Epoch 83, 83/100, training mseloss: 0.222324, testing mseloss: 0.232671
Epoch 84, 84/100, training mseloss: 0.222280, testing mseloss: 0.232861
Epoch 85, 85/100, training mseloss: 0.222242, testing mseloss: 0.232988
Epoch 86, 86/100, training mseloss: 0.222189, testing mseloss: 0.232538
Epoch 87, 87/100, training mseloss: 0.222133, testing mseloss: 0.232364
Epoch 88, 88/100, training mseloss: 0.222108, testing mseloss: 0.232152
Epoch 89, 89/100, training mseloss: 0.222081, testing mseloss: 0.232589
Epoch 90, 90/100, training mseloss: 0.222014, testing mseloss: 0.232502
Epoch 91, 91/100, training mseloss: 0.222027, testing mseloss: 0.232569
Epoch 92, 92/100, training mseloss: 0.221949, testing mseloss: 0.232489
Epoch 93, 93/100, training mseloss: 0.221866, testing mseloss: 0.232118
Epoch 94, 94/100, training mseloss: 0.221872, testing mseloss: 0.232202
Epoch 95, 95/100, training mseloss: 0.221852, testing mseloss: 0.232452
Epoch 96, 96/100, training mseloss: 0.221785, testing mseloss: 0.232167
Epoch 97, 97/100, training mseloss: 0.221775, testing mseloss: 0.232168
Epoch 98, 98/100, training mseloss: 0.221739, testing mseloss: 0.232316
Epoch 99, 99/100, training mseloss: 0.221640, testing mseloss: 0.231944
Epoch 100, 100/100, training mseloss: 0.221642, testing mseloss: 0.232420
