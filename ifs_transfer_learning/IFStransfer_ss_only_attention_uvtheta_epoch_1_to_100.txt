NGPUS = 1
Transfer learning: retraining ERA5 trained attention, vertical=stratosphere_only, horizontal=global model with features uvtheta. CyclicLR scheduler to cycle learning rates between lr_min=0.0001 to lr_max=0.0009.
File name: /scratch/users/ag4680/coarsegrained_ifs_gwmf_helmholtz/NDJF/stratosphere_only_1x1_inputfeatures_u_v_theta_w_N2_uw_vw_era5_training_data_hourly_constant_mu_sigma_scaling.nc
train batch size = 80
validation batch size = 80
Region: 1andes
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
True
True
True
True
True
True
True
True
True
True
model loaded 
 --- model size: 144.17 MBs,
 --- Num params: 37.777 mil. 
Model checkpoint loaded and prepared for re-training
Re-Training final layers...
Epoch 1, 1/100, training mseloss: 0.307386
Epoch 2, 2/100, training mseloss: 0.270620
Epoch 3, 3/100, training mseloss: 0.257713
Epoch 4, 4/100, training mseloss: 0.251551
Epoch 5, 5/100, training mseloss: 0.247361
Epoch 6, 6/100, training mseloss: 0.244901
Epoch 7, 7/100, training mseloss: 0.242821
Epoch 8, 8/100, training mseloss: 0.241384
Epoch 9, 9/100, training mseloss: 0.240249
Epoch 10, 10/100, training mseloss: 0.239071
Epoch 11, 11/100, training mseloss: 0.238394
Epoch 12, 12/100, training mseloss: 0.237591
Epoch 13, 13/100, training mseloss: 0.236820
Epoch 14, 14/100, training mseloss: 0.236492
Epoch 15, 15/100, training mseloss: 0.235788
Epoch 16, 16/100, training mseloss: 0.235258
Epoch 17, 17/100, training mseloss: 0.235099
Epoch 18, 18/100, training mseloss: 0.234435
Epoch 19, 19/100, training mseloss: 0.234069
Epoch 20, 20/100, training mseloss: 0.233980
Epoch 21, 21/100, training mseloss: 0.233359
Epoch 22, 22/100, training mseloss: 0.233108
Epoch 23, 23/100, training mseloss: 0.233025
Epoch 24, 24/100, training mseloss: 0.232461
Epoch 25, 25/100, training mseloss: 0.232305
Epoch 26, 26/100, training mseloss: 0.232192
Epoch 27, 27/100, training mseloss: 0.231705
Epoch 28, 28/100, training mseloss: 0.231625
Epoch 29, 29/100, training mseloss: 0.231457
Epoch 30, 30/100, training mseloss: 0.231058
Epoch 31, 31/100, training mseloss: 0.231045
Epoch 32, 32/100, training mseloss: 0.230805
Epoch 33, 33/100, training mseloss: 0.230497
Epoch 34, 34/100, training mseloss: 0.230540
Epoch 35, 35/100, training mseloss: 0.230224
Epoch 36, 36/100, training mseloss: 0.229998
Epoch 37, 37/100, training mseloss: 0.230068
Epoch 38, 38/100, training mseloss: 0.229701
Epoch 39, 39/100, training mseloss: 0.229545
Epoch 40, 40/100, training mseloss: 0.229612
Epoch 41, 41/100, training mseloss: 0.229224
Epoch 42, 42/100, training mseloss: 0.229127
Epoch 43, 43/100, training mseloss: 0.229164
Epoch 44, 44/100, training mseloss: 0.228785
Epoch 45, 45/100, training mseloss: 0.228743
Epoch 46, 46/100, training mseloss: 0.228732
Epoch 47, 47/100, training mseloss: 0.228391
Epoch 48, 48/100, training mseloss: 0.228396
Epoch 49, 49/100, training mseloss: 0.228323
Epoch 50, 50/100, training mseloss: 0.228035
Epoch 51, 51/100, training mseloss: 0.228086
Epoch 52, 52/100, training mseloss: 0.227937
Epoch 53, 53/100, training mseloss: 0.227716
Epoch 54, 54/100, training mseloss: 0.227805
Epoch 55, 55/100, training mseloss: 0.227579
Epoch 56, 56/100, training mseloss: 0.227419
Epoch 57, 57/100, training mseloss: 0.227524
Epoch 58, 58/100, training mseloss: 0.227245
Epoch 59, 59/100, training mseloss: 0.227138
Epoch 60, 60/100, training mseloss: 0.227235
Epoch 61, 61/100, training mseloss: 0.226929
Epoch 62, 62/100, training mseloss: 0.226871
Epoch 63, 63/100, training mseloss: 0.226938
Epoch 64, 64/100, training mseloss: 0.226633
Epoch 65, 65/100, training mseloss: 0.226621
Epoch 66, 66/100, training mseloss: 0.226640
Epoch 67, 67/100, training mseloss: 0.226362
Epoch 68, 68/100, training mseloss: 0.226390
Epoch 69, 69/100, training mseloss: 0.226352
Epoch 70, 70/100, training mseloss: 0.226116
Epoch 71, 71/100, training mseloss: 0.226186
Epoch 72, 72/100, training mseloss: 0.226077
Epoch 73, 73/100, training mseloss: 0.225893
Epoch 74, 74/100, training mseloss: 0.225996
Epoch 75, 75/100, training mseloss: 0.225816
Epoch 76, 76/100, training mseloss: 0.225684
Epoch 77, 77/100, training mseloss: 0.225801
Epoch 78, 78/100, training mseloss: 0.225568
Epoch 79, 79/100, training mseloss: 0.225485
Epoch 80, 80/100, training mseloss: 0.225591
Epoch 81, 81/100, training mseloss: 0.225330
Epoch 82, 82/100, training mseloss: 0.225291
Epoch 83, 83/100, training mseloss: 0.225368
Epoch 84, 84/100, training mseloss: 0.225104
Epoch 85, 85/100, training mseloss: 0.225107
Epoch 86, 86/100, training mseloss: 0.225140
Epoch 87, 87/100, training mseloss: 0.224897
Epoch 88, 88/100, training mseloss: 0.224938
Epoch 89, 89/100, training mseloss: 0.224916
Epoch 90, 90/100, training mseloss: 0.224709
Epoch 91, 91/100, training mseloss: 0.224787
Epoch 92, 92/100, training mseloss: 0.224700
Epoch 93, 93/100, training mseloss: 0.224538
Epoch 94, 94/100, training mseloss: 0.224647
Epoch 95, 95/100, training mseloss: 0.224492
Epoch 96, 96/100, training mseloss: 0.224377
Epoch 97, 97/100, training mseloss: 0.224496
Epoch 98, 98/100, training mseloss: 0.224291
Epoch 99, 99/100, training mseloss: 0.224220
Epoch 100, 100/100, training mseloss: 0.224329
Training complete.
