NGPUS = 1
Transfer learning: retraining ERA5 trained attention, vertical=stratosphere_only, horizontal=global model with features uvtheta. CyclicLR scheduler to cycle learning rates between lr_min=0.0001 to lr_max=0.0009.
File name: /scratch/users/ag4680/coarsegrained_ifs_gwmf_helmholtz/NDJF/stratosphere_only_1x1_inputfeatures_u_v_theta_w_N2_uw_vw_era5_training_data_hourly_constant_mu_sigma_scaling.nc
train batch size = 80
validation batch size = 80
Region: 1andes
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
True
True
model loaded 
 --- model size: 144.17 MBs,
 --- Num params: 37.777 mil. 
Model checkpoint loaded and prepared for re-training
Re-Training final layers...
Epoch 1, 1/100, training mseloss: 0.320866
Epoch 2, 2/100, training mseloss: 0.300967
Epoch 3, 3/100, training mseloss: 0.297835
Epoch 4, 4/100, training mseloss: 0.295774
Epoch 5, 5/100, training mseloss: 0.293528
Epoch 6, 6/100, training mseloss: 0.292079
Epoch 7, 7/100, training mseloss: 0.290578
Epoch 8, 8/100, training mseloss: 0.289279
Epoch 9, 9/100, training mseloss: 0.288304
Epoch 10, 10/100, training mseloss: 0.287259
Epoch 11, 11/100, training mseloss: 0.286400
Epoch 12, 12/100, training mseloss: 0.285687
Epoch 13, 13/100, training mseloss: 0.284902
Epoch 14, 14/100, training mseloss: 0.284310
Epoch 15, 15/100, training mseloss: 0.283743
Epoch 16, 16/100, training mseloss: 0.283141
Epoch 17, 17/100, training mseloss: 0.282719
Epoch 18, 18/100, training mseloss: 0.282243
Epoch 19, 19/100, training mseloss: 0.281773
Epoch 20, 20/100, training mseloss: 0.281460
Epoch 21, 21/100, training mseloss: 0.281045
Epoch 22, 22/100, training mseloss: 0.280677
Epoch 23, 23/100, training mseloss: 0.280430
Epoch 24, 24/100, training mseloss: 0.280061
Epoch 25, 25/100, training mseloss: 0.279774
Epoch 26, 26/100, training mseloss: 0.279564
Epoch 27, 27/100, training mseloss: 0.279236
Epoch 28, 28/100, training mseloss: 0.279013
Epoch 29, 29/100, training mseloss: 0.278821
Epoch 30, 30/100, training mseloss: 0.278531
Epoch 31, 31/100, training mseloss: 0.278361
Epoch 32, 32/100, training mseloss: 0.278173
Epoch 33, 33/100, training mseloss: 0.277920
Epoch 34, 34/100, training mseloss: 0.277792
Epoch 35, 35/100, training mseloss: 0.277601
Epoch 36, 36/100, training mseloss: 0.277385
Epoch 37, 37/100, training mseloss: 0.277286
Epoch 38, 38/100, training mseloss: 0.277092
Epoch 39, 39/100, training mseloss: 0.276911
Epoch 40, 40/100, training mseloss: 0.276830
Epoch 41, 41/100, training mseloss: 0.276636
Epoch 42, 42/100, training mseloss: 0.276488
Epoch 43, 43/100, training mseloss: 0.276414
Epoch 44, 44/100, training mseloss: 0.276222
Epoch 45, 45/100, training mseloss: 0.276105
Epoch 46, 46/100, training mseloss: 0.276030
Epoch 47, 47/100, training mseloss: 0.275845
Epoch 48, 48/100, training mseloss: 0.275756
Epoch 49, 49/100, training mseloss: 0.275673
Epoch 50, 50/100, training mseloss: 0.275501
Epoch 51, 51/100, training mseloss: 0.275436
Epoch 52, 52/100, training mseloss: 0.275340
Epoch 53, 53/100, training mseloss: 0.275186
Epoch 54, 54/100, training mseloss: 0.275142
Epoch 55, 55/100, training mseloss: 0.275030
Epoch 56, 56/100, training mseloss: 0.274897
Epoch 57, 57/100, training mseloss: 0.274867
Epoch 58, 58/100, training mseloss: 0.274742
Epoch 59, 59/100, training mseloss: 0.274631
Epoch 60, 60/100, training mseloss: 0.274607
Epoch 61, 61/100, training mseloss: 0.274474
Epoch 62, 62/100, training mseloss: 0.274385
Epoch 63, 63/100, training mseloss: 0.274360
Epoch 64, 64/100, training mseloss: 0.274222
Epoch 65, 65/100, training mseloss: 0.274156
Epoch 66, 66/100, training mseloss: 0.274123
Epoch 67, 67/100, training mseloss: 0.273987
Epoch 68, 68/100, training mseloss: 0.273942
Epoch 69, 69/100, training mseloss: 0.273896
Epoch 70, 70/100, training mseloss: 0.273767
Epoch 71, 71/100, training mseloss: 0.273741
Epoch 72, 72/100, training mseloss: 0.273678
Epoch 73, 73/100, training mseloss: 0.273563
Epoch 74, 74/100, training mseloss: 0.273551
Epoch 75, 75/100, training mseloss: 0.273472
Epoch 76, 76/100, training mseloss: 0.273373
Epoch 77, 77/100, training mseloss: 0.273371
Epoch 78, 78/100, training mseloss: 0.273276
Epoch 79, 79/100, training mseloss: 0.273196
Epoch 80, 80/100, training mseloss: 0.273196
Epoch 81, 81/100, training mseloss: 0.273091
Epoch 82, 82/100, training mseloss: 0.273030
Epoch 83, 83/100, training mseloss: 0.273027
Epoch 84, 84/100, training mseloss: 0.272916
Epoch 85, 85/100, training mseloss: 0.272874
Epoch 86, 86/100, training mseloss: 0.272861
Epoch 87, 87/100, training mseloss: 0.272750
Epoch 88, 88/100, training mseloss: 0.272727
Epoch 89, 89/100, training mseloss: 0.272700
Epoch 90, 90/100, training mseloss: 0.272594
Epoch 91, 91/100, training mseloss: 0.272586
Epoch 92, 92/100, training mseloss: 0.272543
Epoch 93, 93/100, training mseloss: 0.272448
Epoch 94, 94/100, training mseloss: 0.272453
Epoch 95, 95/100, training mseloss: 0.272391
Epoch 96, 96/100, training mseloss: 0.272312
Epoch 97, 97/100, training mseloss: 0.272324
Epoch 98, 98/100, training mseloss: 0.272247
Epoch 99, 99/100, training mseloss: 0.272184
Epoch 100, 100/100, training mseloss: 0.272198
Training complete.
