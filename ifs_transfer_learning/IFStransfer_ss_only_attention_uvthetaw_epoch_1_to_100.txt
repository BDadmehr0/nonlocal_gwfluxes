NGPUS = 1
Transfer learning: retraining ERA5 trained attention, vertical=stratosphere_only, horizontal=global model with features uvthetaw. CyclicLR scheduler to cycle learning rates between lr_min=0.0001 to lr_max=0.0009.
File name: /scratch/users/ag4680/coarsegrained_ifs_gwmf_helmholtz/NDJF/stratosphere_only_1x1_inputfeatures_u_v_theta_w_N2_uw_vw_era5_training_data_hourly_constant_mu_sigma_scaling.nc
train batch size = 80
validation batch size = 80
Region: 1andes
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
True
True
True
True
True
True
True
True
True
True
model loaded 
 --- model size: 144.30 MBs,
 --- Num params: 37.812 mil. 
Model checkpoint loaded and prepared for re-training
Re-Training final layers...
Epoch 1, 1/100, training mseloss: 0.289511
Epoch 2, 2/100, training mseloss: 0.255529
Epoch 3, 3/100, training mseloss: 0.244057
Epoch 4, 4/100, training mseloss: 0.240095
Epoch 5, 5/100, training mseloss: 0.235274
Epoch 6, 6/100, training mseloss: 0.232888
Epoch 7, 7/100, training mseloss: 0.230888
Epoch 8, 8/100, training mseloss: 0.229091
Epoch 9, 9/100, training mseloss: 0.227970
Epoch 10, 10/100, training mseloss: 0.226473
Epoch 11, 11/100, training mseloss: 0.225467
Epoch 12, 12/100, training mseloss: 0.224676
Epoch 13, 13/100, training mseloss: 0.223562
Epoch 14, 14/100, training mseloss: 0.222927
Epoch 15, 15/100, training mseloss: 0.222216
Epoch 16, 16/100, training mseloss: 0.221404
Epoch 17, 17/100, training mseloss: 0.220998
Epoch 18, 18/100, training mseloss: 0.220303
Epoch 19, 19/100, training mseloss: 0.219719
Epoch 20, 20/100, training mseloss: 0.219447
Epoch 21, 21/100, training mseloss: 0.218775
Epoch 22, 22/100, training mseloss: 0.218347
Epoch 23, 23/100, training mseloss: 0.218142
Epoch 24, 24/100, training mseloss: 0.217517
Epoch 25, 25/100, training mseloss: 0.217204
Epoch 26, 26/100, training mseloss: 0.217012
Epoch 27, 27/100, training mseloss: 0.216459
Epoch 28, 28/100, training mseloss: 0.216234
Epoch 29, 29/100, training mseloss: 0.216017
Epoch 30, 30/100, training mseloss: 0.215551
Epoch 31, 31/100, training mseloss: 0.215401
Epoch 32, 32/100, training mseloss: 0.215132
Epoch 33, 33/100, training mseloss: 0.214760
Epoch 34, 34/100, training mseloss: 0.214675
Epoch 35, 35/100, training mseloss: 0.214348
Epoch 36, 36/100, training mseloss: 0.214065
Epoch 37, 37/100, training mseloss: 0.214027
Epoch 38, 38/100, training mseloss: 0.213651
Epoch 39, 39/100, training mseloss: 0.213442
Epoch 40, 40/100, training mseloss: 0.213428
Epoch 41, 41/100, training mseloss: 0.213031
Epoch 42, 42/100, training mseloss: 0.212878
Epoch 43, 43/100, training mseloss: 0.212857
Epoch 44, 44/100, training mseloss: 0.212470
Epoch 45, 45/100, training mseloss: 0.212363
Epoch 46, 46/100, training mseloss: 0.212310
Epoch 47, 47/100, training mseloss: 0.211959
Epoch 48, 48/100, training mseloss: 0.211894
Epoch 49, 49/100, training mseloss: 0.211789
Epoch 50, 50/100, training mseloss: 0.211492
Epoch 51, 51/100, training mseloss: 0.211467
Epoch 52, 52/100, training mseloss: 0.211297
Epoch 53, 53/100, training mseloss: 0.211064
Epoch 54, 54/100, training mseloss: 0.211078
Epoch 55, 55/100, training mseloss: 0.210843
Epoch 56, 56/100, training mseloss: 0.210671
Epoch 57, 57/100, training mseloss: 0.210713
Epoch 58, 58/100, training mseloss: 0.210427
Epoch 59, 59/100, training mseloss: 0.210306
Epoch 60, 60/100, training mseloss: 0.210354
Epoch 61, 61/100, training mseloss: 0.210042
Epoch 62, 62/100, training mseloss: 0.209962
Epoch 63, 63/100, training mseloss: 0.209991
Epoch 64, 64/100, training mseloss: 0.209684
Epoch 65, 65/100, training mseloss: 0.209638
Epoch 66, 66/100, training mseloss: 0.209629
Epoch 67, 67/100, training mseloss: 0.209349
Epoch 68, 68/100, training mseloss: 0.209336
Epoch 69, 69/100, training mseloss: 0.209271
Epoch 70, 70/100, training mseloss: 0.209036
Epoch 71, 71/100, training mseloss: 0.209056
Epoch 72, 72/100, training mseloss: 0.208927
Epoch 73, 73/100, training mseloss: 0.208746
Epoch 74, 74/100, training mseloss: 0.208798
Epoch 75, 75/100, training mseloss: 0.208604
Epoch 76, 76/100, training mseloss: 0.208476
Epoch 77, 77/100, training mseloss: 0.208547
Epoch 78, 78/100, training mseloss: 0.208304
Epoch 79, 79/100, training mseloss: 0.208220
Epoch 80, 80/100, training mseloss: 0.208293
Epoch 81, 81/100, training mseloss: 0.208024
Epoch 82, 82/100, training mseloss: 0.207976
Epoch 83, 83/100, training mseloss: 0.208028
Epoch 84, 84/100, training mseloss: 0.207758
Epoch 85, 85/100, training mseloss: 0.207741
Epoch 86, 86/100, training mseloss: 0.207752
Epoch 87, 87/100, training mseloss: 0.207507
Epoch 88, 88/100, training mseloss: 0.207520
Epoch 89, 89/100, training mseloss: 0.207476
Epoch 90, 90/100, training mseloss: 0.207273
Epoch 91, 91/100, training mseloss: 0.207315
Epoch 92, 92/100, training mseloss: 0.207207
Epoch 93, 93/100, training mseloss: 0.207055
Epoch 94, 94/100, training mseloss: 0.207124
Epoch 95, 95/100, training mseloss: 0.206953
Epoch 96, 96/100, training mseloss: 0.206849
Epoch 97, 97/100, training mseloss: 0.206936
Epoch 98, 98/100, training mseloss: 0.206717
Epoch 99, 99/100, training mseloss: 0.206654
Epoch 100, 100/100, training mseloss: 0.206740
Training complete.
